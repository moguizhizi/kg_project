"""
xlsx_loader.py

èŒè´£ï¼š
- ä» XLSX æ–‡ä»¶åŠ è½½æ•°æ®
- åšæœ€åŸºç¡€ã€å¯å¤ç”¨çš„æ•°æ®æ¸…æ´—
- è¾“å‡ºç»Ÿä¸€çš„ List[Dict] ç»“æ„ï¼Œä¾›ä¸‰å…ƒç»„æŠ½å–ä½¿ç”¨
"""

from __future__ import annotations

import os
import logging
from typing import List, Dict, Optional, Union

import pandas as pd

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
)
logger = logging.getLogger(__name__)


# =========================
# åŸºç¡€åŠ è½½
# =========================


def load_xlsx(
    path: str,
    sheet_name: Union[str, int, None] = 0,
    header: int = 0,
) -> pd.DataFrame:
    """
    åŠ è½½ XLSX æ–‡ä»¶ï¼Œè¿”å›åŸå§‹ DataFrame
    """
    logger.info(f"Loading XLSX file: {path}, sheet={sheet_name}")

    if not os.path.exists(path):
        logger.error(f"XLSX file not found: {path}")
        raise FileNotFoundError(f"XLSX file not found: {path}")

    df = pd.read_excel(path, sheet_name=sheet_name, header=header)

    logger.info(f"Loaded DataFrame with shape {df.shape}")
    return df


# =========================
# å­—æ®µä¸æ•°æ®æ¸…æ´—
# =========================


def normalize_columns(
    df: pd.DataFrame,
    column_mapping: Optional[Dict[str, str]] = None,
    strip_whitespace: bool = True,
) -> pd.DataFrame:
    """
    ç»Ÿä¸€å­—æ®µåï¼ˆåˆ«å / ç©ºæ ¼ / å…¨è§’é—®é¢˜ï¼‰
    """
    df = df.copy()

    if strip_whitespace:
        old_cols = list(df.columns)
        df.columns = [str(c).strip() for c in df.columns]
        if old_cols != list(df.columns):
            logger.info("Stripped whitespace from column names")

    if column_mapping:
        logger.info(f"Applying column mapping: {column_mapping}")
        df = df.rename(columns=column_mapping)

    logger.debug(f"Final columns: {list(df.columns)}")
    return df


def drop_empty_rows(df: pd.DataFrame) -> pd.DataFrame:
    """
    åˆ é™¤å…¨ä¸ºç©ºçš„è¡Œ
    """
    before = len(df)
    df = df.dropna(how="all")
    after = len(df)

    if before != after:
        logger.info(f"Dropped {before - after} empty rows")

    return df


def fill_na_values(df: pd.DataFrame) -> pd.DataFrame:
    """
    ä¿æŒ DataFrame å†…éƒ¨ä¸º NaNï¼Œ
    åœ¨å¯¼å‡º records æ—¶å†è½¬ä¸º None
    """
    logger.info("Keeping NA values as NaN (will normalize at export stage)")
    return df


def parse_date_fields(
    df: pd.DataFrame,
    date_fields: List[str],
    date_format: Optional[str] = None,
) -> pd.DataFrame:
    """
    å°†æ—¥æœŸå­—æ®µç»Ÿä¸€è½¬ä¸º ISO æ ¼å¼å­—ç¬¦ä¸²
    """
    df = df.copy()

    for field in date_fields:
        if field not in df.columns:
            logger.warning(f"Date field not found, skip: {field}")
            continue

        logger.info(f"Parsing date field: {field}")
        df[field] = pd.to_datetime(
            df[field],
            format=date_format,
            errors="coerce",
        ).dt.strftime("%Y-%m-%d")

    return df


def split_multi_value_fields(
    df: pd.DataFrame,
    fields: List[str],
    sep: str = ",",
) -> pd.DataFrame:
    """
    æ‹†åˆ†å¤šå€¼å­—æ®µï¼ˆå¦‚ ç–¾ç—…ï¼šA,B,Cï¼‰
    æ³¨æ„ï¼šè¿™é‡Œåªåšå­—ç¬¦ä¸² â†’ listï¼Œä¸åšè¡Œå±•å¼€
    """
    df = df.copy()

    for field in fields:
        if field not in df.columns:
            logger.warning(f"Multi-value field not found, skip: {field}")
            continue

        logger.info(f"Splitting multi-value field: {field}")

        def _split(val):
            if pd.isna(val):
                return []
            if isinstance(val, str):
                return [v.strip() for v in val.split(sep) if v.strip()]
            return [val]

        df[field] = df[field].apply(_split)

    return df


# =========================
# æ ¡éªŒ
# =========================


def validate_schema(df: pd.DataFrame, required_fields: List[str]) -> None:
    """
    æ ¡éªŒå¿…éœ€å­—æ®µæ˜¯å¦å­˜åœ¨
    """
    logger.info(f"Validating required fields: {required_fields}")

    missing = [f for f in required_fields if f not in df.columns]
    if missing:
        logger.error(f"Missing required fields: {missing}")
        raise ValueError(f"Missing required fields: {missing}")

    logger.info("Schema validation passed")


# =========================
# ç»Ÿä¸€å‡ºå£
# =========================


def xlsx_to_records(
    path: str,
    sheet_name: Union[str, int, None] = 0,
    column_mapping: Optional[Dict[str, str]] = None,
    date_fields: Optional[List[str]] = None,
    multi_value_fields: Optional[List[str]] = None,
    required_fields: Optional[List[str]] = None,
) -> List[Dict]:
    """
    XLSX â†’ å¹²å‡€çš„ recordsï¼ˆList[Dict]ï¼‰
    """
    logger.info("Starting XLSX to records pipeline")

    df = load_xlsx(path, sheet_name=sheet_name)
    df = normalize_columns(df, column_mapping=column_mapping)
    df = drop_empty_rows(df)
    df = fill_na_values(df)

    if not df.empty:
        logger.info(f"Sample row (first): {df.iloc[0].to_dict()}")

    # æ ¡éªŒï¼ˆå¯é€‰ï¼‰
    # if required_fields:
    #     validate_schema(df, required_fields)

    # æ—¥æœŸè§£æï¼ˆå¯é€‰ï¼‰
    # if date_fields:
    #     df = parse_date_fields(df, date_fields)

    # å¤šå€¼å­—æ®µæ‹†åˆ†ï¼ˆå¯é€‰ï¼‰
    # if multi_value_fields:
    #     df = split_multi_value_fields(df, multi_value_fields)

    # ğŸ”¥ æ ¸å¿ƒï¼šå¯¼å‡º recordsï¼Œå¹¶æŠŠ NaN â†’ None
    records = df.where(pd.notnull(df), None).to_dict(orient="records")

    logger.info(f"Generated {len(records)} records")
    return records


if __name__ == "__main__":
    xlsx_path = "/home/temp/dataset/temp.xlsx"

    records = xlsx_to_records(
        path=xlsx_path,
        sheet_name="Sheet1",
    )

    logger.info(f"Total records loaded: {len(records)}")

    if records:
        logger.info(f"First record: {records[0]}")
